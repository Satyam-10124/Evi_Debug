{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch fetch Job **logs** and **status** to files\n",
        "\n",
        "Paste your job IDs, run all cells, and this notebook will:\n",
        "1. Call:\n",
        "   - `https://acadcodegen-production.up.railway.app/api/job/<JOB_ID>/logs`\n",
        "   - `https://acadcodegen-production.up.railway.app/api/job/<JOB_ID>/status`\n",
        "2. Save results under `outputs/` as JSON (and raw text if needed)\n",
        "3. Produce a `summary.csv` with high‚Äëlevel info\n",
        "\n",
        "**Tip:** If an endpoint returns non‚ÄëJSON (e.g. plain text logs), this notebook saves both a JSON wrapper and a `*.txt` raw copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üîß Setup\n",
        "import os, json, time, csv\n",
        "from typing import Any, Dict, Tuple\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"https://acadcodegen-production.up.railway.app/api/job\"  # no trailing slash\n",
        "OUTDIR = \"outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "def make_session(total_retries: int = 3, backoff: float = 0.5, timeout: int = 30):\n",
        "    retry = Retry(\n",
        "        total=total_retries,\n",
        "        read=total_retries,\n",
        "        connect=total_retries,\n",
        "        backoff_factor=backoff,\n",
        "        status_forcelist=(429, 500, 502, 503, 504),\n",
        "        allowed_methods=(\"GET\",),\n",
        "        raise_on_status=False,\n",
        "    )\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"Accept\": \"*/*\", \"User-Agent\": \"colab-job-fetcher/1.0\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
        "    s.request_timeout = timeout\n",
        "    return s\n",
        "\n",
        "SESSION = make_session()\n",
        "\n",
        "def robust_get(url: str) -> Tuple[int, str, Any]:\n",
        "    \"\"\"GET url, return (status_code, content_type, parsed_or_text).\"\"\"\n",
        "    try:\n",
        "        r = SESSION.get(url, timeout=SESSION.request_timeout)\n",
        "        ctype = r.headers.get(\"Content-Type\", \"\")\n",
        "        # Attempt JSON first, fallback to text\n",
        "        data: Any\n",
        "        if \"application/json\" in ctype:\n",
        "            try:\n",
        "                data = r.json()\n",
        "            except Exception:\n",
        "                data = r.text\n",
        "        else:\n",
        "            # Some log endpoints return text/plain; keep raw text as well\n",
        "            try:\n",
        "                data = r.json()\n",
        "            except Exception:\n",
        "                data = r.text\n",
        "        return r.status_code, ctype, data\n",
        "    except requests.RequestException as e:\n",
        "        return 0, \"\", {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ‚úèÔ∏è Paste your Job IDs here\n",
        "job_ids = [\n",
        "    # Examples from your message (you can replace/extend this list):\n",
        "    \"ai_pipeline_d9de2314-d984-45b6-b130-2bc0d0886fa2\",  # BadgeRegistry (Error: context is not defined)\n",
        "    \"ai_pipeline_b8c58f70-7f3d-4e4b-96c1-cad82372af3b\",  # ContentRegistry (Error: context is not defined)\n",
        "    \"ai_pipeline_0d8a9752-b54b-4fd8-a6a6-e607b3c21206\",  # WhitelistRegistry (Error: context is not defined)\n",
        "    \"ai_pipeline_9d3e8c13-7fde-4d5d-ae19-846fcf66a905\",  # SimpleLottery (Error: context is not defined)\n",
        "]\n",
        "\n",
        "# Optional small delay between requests to be gentle on the API\n",
        "per_request_sleep_seconds = 0.1  #@param {type:\"number\"}\n",
        "\n",
        "print(f\"Loaded {len(job_ids)} job IDs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üöÄ Fetch logs & status for each job and save to files\n",
        "summary_rows = []\n",
        "\n",
        "for job_id in job_ids:\n",
        "    logs_url = f\"{BASE}/{job_id}/logs\"\n",
        "    status_url = f\"{BASE}/{job_id}/status\"\n",
        "\n",
        "    print(f\"\\n==> {job_id}\")\n",
        "    print(\"GET\", logs_url)\n",
        "    sc_logs, ct_logs, data_logs = robust_get(logs_url)\n",
        "    print(\"   status:\", sc_logs, \"content-type:\", ct_logs)\n",
        "\n",
        "    # Save logs (JSON if possible) + raw text fallback\n",
        "    logs_json_path = os.path.join(OUTDIR, f\"{job_id}_logs.json\")\n",
        "    logs_txt_path = os.path.join(OUTDIR, f\"{job_id}_logs.txt\")\n",
        "    try:\n",
        "        with open(logs_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"url\": logs_url, \"status_code\": sc_logs, \"content_type\": ct_logs, \"data\": data_logs}, f, ensure_ascii=False, indent=2)\n",
        "        if isinstance(data_logs, str):\n",
        "            with open(logs_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(data_logs)\n",
        "    except Exception as e:\n",
        "        print(\"   ‚ö†Ô∏è Failed to save logs:\", e)\n",
        "\n",
        "    time.sleep(per_request_sleep_seconds)\n",
        "\n",
        "    print(\"GET\", status_url)\n",
        "    sc_status, ct_status, data_status = robust_get(status_url)\n",
        "    print(\"   status:\", sc_status, \"content-type:\", ct_status)\n",
        "\n",
        "    status_json_path = os.path.join(OUTDIR, f\"{job_id}_status.json\")\n",
        "    status_txt_path = os.path.join(OUTDIR, f\"{job_id}_status.txt\")\n",
        "    try:\n",
        "        with open(status_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"url\": status_url, \"status_code\": sc_status, \"content_type\": ct_status, \"data\": data_status}, f, ensure_ascii=False, indent=2)\n",
        "        if isinstance(data_status, str):\n",
        "            with open(status_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(data_status)\n",
        "    except Exception as e:\n",
        "        print(\"   ‚ö†Ô∏è Failed to save status:\", e)\n",
        "\n",
        "    # Best-effort extraction of a status field if JSON\n",
        "    status_value = None\n",
        "    if isinstance(data_status, dict):\n",
        "        for key in (\"status\", \"state\", \"job_status\", \"result\"):\n",
        "            if key in data_status:\n",
        "                status_value = data_status[key]\n",
        "                break\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"job_id\": job_id,\n",
        "        \"logs_http\": sc_logs,\n",
        "        \"status_http\": sc_status,\n",
        "        \"status_value\": status_value,\n",
        "        \"logs_file\": os.path.basename(logs_json_path),\n",
        "        \"status_file\": os.path.basename(status_json_path),\n",
        "    })\n",
        "\n",
        "print(\"\\nSaving summary...\")\n",
        "summary_csv = os.path.join(OUTDIR, \"summary.csv\")\n",
        "with open(summary_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    import csv\n",
        "    fieldnames = list(summary_rows[0].keys()) if summary_rows else [\n",
        "        \"job_id\",\"logs_http\",\"status_http\",\"status_value\",\"logs_file\",\"status_file\"\n",
        "    ]\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for row in summary_rows:\n",
        "        writer.writerow(row)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(summary_rows)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üì¶ (Optional) Zip the outputs/ folder for download\n",
        "import shutil\n",
        "zip_path = shutil.make_archive(\"job_results\", \"zip\", root_dir=OUTDIR)\n",
        "print(\"Created:\", zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "- If you see HTTP 401/403, your endpoint may require auth; add headers/tokens where the session is created.\n",
        "- If logs come back as text, check the `*_logs.txt` alongside the JSON.\n",
        "- Increase `total_retries` or `timeout` in `make_session` for flaky connections.\n",
        "- If your base URL changes, edit the `BASE` constant in the setup cell."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fetch_Job_Logs_and_Status.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}